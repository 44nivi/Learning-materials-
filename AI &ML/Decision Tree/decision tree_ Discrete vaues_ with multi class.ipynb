{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define a class for a decision tree node\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, subtrees=None, value=None):\n",
    "        self.feature_index = feature_index  # Index of the feature used for splitting\n",
    "        self.threshold = threshold  # Threshold value for splitting\n",
    "        self.subtrees = subtrees  # Subtrees for different feature values\n",
    "        self.value = value  # Value to return if this node is a leaf node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth  # Maximum depth of the decision tree\n",
    "        self.tree = None  # Root node of the decision tree\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "            n_features = X.shape[1]\n",
    "            best_gini = float('inf')\n",
    "            best_feature_index = None\n",
    "            best_threshold = None\n",
    "\n",
    "            # Calculate Gini impurity for each feature and threshold\n",
    "            for feature_index in range(n_features):\n",
    "                thresholds = np.unique(X[:, feature_index])\n",
    "                for threshold in thresholds:\n",
    "                    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "                    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "                    gini_left = self.calculate_gini(y[left_indices])\n",
    "                    gini_right = self.calculate_gini(y[right_indices])\n",
    "\n",
    "                    gini = (len(left_indices) / len(y)) * gini_left + (len(right_indices) / len(y)) * gini_right\n",
    "\n",
    "                    if gini < best_gini:\n",
    "                        best_gini = gini\n",
    "                        best_feature_index = feature_index\n",
    "                        best_threshold = threshold\n",
    "\n",
    "            return best_feature_index, best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(self, y):\n",
    "            unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "            gini = 1\n",
    "            for class_count in class_counts:\n",
    "                proportion = class_count / len(y)\n",
    "                gini -= proportion ** 2\n",
    "            return gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(self, X, y, depth=0):\n",
    "        # Stopping criteria: if maximum depth is reached or all instances belong to the same class\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return DecisionTreeNode(value=np.argmax(np.bincount(y)))\n",
    "\n",
    "        best_feature_index, best_threshold = self.find_best_split(X, y)\n",
    "\n",
    "        # Stopping criteria: if no best split is found\n",
    "        if best_feature_index is None:\n",
    "            return DecisionTreeNode(value=np.argmax(np.bincount(y)))\n",
    "\n",
    "        # Split the dataset based on the best split found\n",
    "        feature_values = np.unique(X[:, best_feature_index])\n",
    "        subtrees = {}\n",
    "        for value in feature_values:\n",
    "            indices = np.where(X[:, best_feature_index] == value)[0]\n",
    "            subtrees[value] = self.build_tree(X[indices], y[indices], depth + 1)\n",
    "\n",
    "        return DecisionTreeNode(feature_index=best_feature_index, threshold=best_threshold,\n",
    "                                subtrees=subtrees)\n",
    "def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "                self.tree = self.build_tree(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            node = self.tree\n",
    "            while node.subtrees:\n",
    "                value = instance[node.feature_index]\n",
    "                if value in node.subtrees:\n",
    "                    node = node.subtrees[value]\n",
    "                else:\n",
    "                    break\n",
    "            predictions.append(node.value)\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(node, depth=0):\n",
    "    if node is None:\n",
    "        return\n",
    "    print(\"  \" * depth, end=\"\")\n",
    "    if node.feature_index is None:\n",
    "        print(\"Leaf, Predicted Class:\", node.value)\n",
    "    else:\n",
    "        print(\"Feature:\", features[node.feature_index], \", Threshold:\", node.threshold)\n",
    "        print(\"  \" * depth, \"Subtrees:\")\n",
    "        for value, subtree in node.subtrees.items():\n",
    "            print(\"  \" * (depth + 1), f\"Value: {value}\")\n",
    "            visualize_tree(subtree, depth + 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyDecisionTreeClassifier' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Use the custom DecisionTreeClassifier\u001b[39;00m\n\u001b[0;32m     18\u001b[0m my_clf \u001b[38;5;241m=\u001b[39m MyDecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mmy_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(X_train, y_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyDecisionTreeClassifier' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('newdata.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['Other online courses', 'Student background', 'Working Status']\n",
    "X = df[features].values\n",
    "y = df['Exam Result'].values\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split Data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, y_train = X,y\n",
    "\n",
    "# Use the custom DecisionTreeClassifier\n",
    "my_clf = MyDecisionTreeClassifier(max_depth=4)\n",
    "my_clf.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
